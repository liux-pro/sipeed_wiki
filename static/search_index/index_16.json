{"/ai/en/index.html": {"title": "AI Guide", "content": "---\ntitle: AI Guide\ndate: 2022-09-15\n---\n\n\n## About this document\n\n`Sipeed` has launched a series of `AI` development boards, including:\n* `Maix-I(M1)` series of `MaixBit`, `Maixduino` `M1s-Dock` and other microcontroller development boards with hardware `AI` acceleration.\n* Cost-effective `SOC` development boards with `AI` hardware acceleration, such as `M2-Dock` `MaixSense` of the `Maix-II(M2)` series.\n* High-performance `SOC` development boards such as `M3 AX-Pi` of the `Maix-III(M3)` series.\n\nIn order to popularize the application of `AI` on edge devices, `Sipeed` has developed easy-to-use `MaixPy` and `MaixPy3` SDKs, and supplied [MaixHub](https://maixhub.com/) platform to make developers can easily train models without AI programming foundation and AI training experience.\n\nOn this basis, this document is dedicated to providing developers with an `AI` development guide, the purpose is to allow novices to quickly get started with `AI` applications, or developers who have mastered `AI` related knowledge to quickly apply the results to the device or on the product.\nThe content includes but is not limited to:\n* Basic knowledge of AI\n* Model training guide\n* AI tutorial recommendation\n* AI interesting project recommendation\n* Commonly used tools\n* Edge Device Deployment Guide\n\n\n## Participate in contribution\n\nThe content of the document will be continuously updated, and everyone is welcome to participate in the content writing. How to participate:\n\n* You can directly click the `Edit this page` button in the upper right corner of the document to jump to `GitHub` to edit and submit a `PR` (for specific methods, see [Contribution Documentation](/share_docs/zh/) or search engine search `GitHub' how to submit a PR`).\n* You can also directly send suggestions for changes or submit your manuscript to `support@sipeed.com`, the title should start with `[WiKi contribution]`, the text needs to indicate the author, the modified content and where to be modified, so that we can quickly transfer your Content updated to documentation."}, "/ai/en/deploy/index.html": {"title": "Deploy models to edge devices methods", "content": "---\ntitle: Deploy models to edge devices methods\ndate: 2022-09-21\nclass: heading_no_counter\n---\n\n<div id=\"maixhub\">\n    <a href=\"https://maixhub.com/model/zoo\">Find</a>or<a href=\"https://maixhub.com/model/zoo/share\">share</a> edge models on MaixHub\n</div>\n\n<div id=\"deploy_items\">\n    <a href=\"./k210.html\">\n        <div class=\"card\">\n            <img src=\"/hardware/zh/maix/assets/dk_board/maix_duino/maixduino_0.png\" alt=\"K210 model convert and deployment\">\n            <div class=\"card_info card_red\">\n                <h2>Maix-I Series K210</h2>\n                <div class=\"brief\">\n                    <div>MCU with hardware AI acceleration</div>\n                    <div>1Tops@INT8, limited operators support</div>\n                </div>\n            </div>\n        </div>\n    </a>\n    <a href=\"./v831.html\">\n        <div class=\"card\">\n            <img src=\"/hardware/assets/maixII/m2dock.jpg\" alt=\"V831 model convert and deployment\">\n            <div class=\"card_info card_blue\">\n                <h2>Maix-II Series v831</h2>\n                <div class=\"brief\">\n                    <div>SOC with hardware AI acceleration, Linux OS</div>\n                    <div>0.2Tops@INT8, limited operators support</div>\n                </div>\n            </div>\n        </div>\n    </a>\n    <a href=\"./tinymaix.html\">\n        <div class=\"card\" style=\"background-color: #fafbfe\">\n            <img src=\"../../assets/m0_small.png\" alt=\"TinyMaix model convert and deployment\">\n            <div class=\"card_info card_green\">\n                <h2>TinyMaix platform</h2>\n                <div class=\"brief\">\n                    <div>For all MCU with optimization for many ARCH</div>\n                    <div>Depends on CPU, limited operators support</div>\n                </div>\n            </div>\n        </div>\n    </a>\n    <a href=\"./ax-pi.html\">\n        <div class=\"card\" style=\"background-color: #fafbfe\">\n            <img src=\"../../assets/maix-iii-small.png\" alt=\"AX-Pi model convert and deployment\">\n            <div class=\"card_info card_purple\">\n                <h2>Maix-III Series AX-Pi</h2>\n                <div class=\"brief\">\n                    <div>High computing power, special AI ISP</div>\n                    <div>Max 3.6Tops@INT8, many operators</div>\n                </div>\n            </div>\n        </div>\n    </a>\n</div>\n\n<style>\n#deploy_items {\n    display: flex;\n    justify-content: space-evenly;\n    flex-wrap: wrap;\n    margin: 0 -10px;\n}\n#deploy_items a:hover {\n    background-color: transparent;\n}\n#deploy_items > a {\n    margin: 1em;\n}\n.card {\n    display: flex;\n    flex-direction: column;\n    justify-content: space-between;\n    align-items: center;\n    box-shadow: 5px 6px 20px 4px  rgba(0, 0, 0, 0.1);\n    border-radius: 0.6rem;\n    transition: 0.4s;\n    background: white;\n}\n.card:hover {\n    box-shadow: 5px 6px 40px 4px  rgba(0, 0, 0, 0.1);\n    scale: 1.05;\n}\n.card_info {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    border-radius: 0 0 0.6rem 0.6rem;\n}\n.card img {\n    height: 10em;\n    width: 14em;\n    object-fit: cover;\n}\n.card_info > h2 {\n    font-size: 1.2em;\n    margin: 0.2em;\n    padding: 0.2em 1em;\n}\n.card_info > .brief {\n    margin: 0.2em;\n    padding: 0.2em 1em;\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n}\n.card_red {\n    background-color: #ffcdd2;\n    color: #cf4f5a;\n}\n.card_blue {\n    background-color: #90caf9;\n    color: #105aa9;\n}\n.card_green {\n    background-color: #b2dfdb;\n    color: #009688;\n}\n.card_purple {\n    background-color: #d1c4e9;\n    color: #673ab7;\n}\n#maixhub {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    margin: 1em 0;\n    width: 100%;\n    background-color: #f5f5f5;\n    color: #727272;\n    border-radius: 0.6rem;\n    padding: 1em;\n}\n.dark #maixhub {\n    background-color: #2d2d2d;\n    color: #bfbfbf;\n}\n.dark .card_blue {\n    background-color: #003c6c;\n    color: #ffffffba;\n}\n.dark .card_red {\n    background-color: #5a0000;\n    color: #ffffffba;\n}\n.dark .card_green {\n    background-color: #004e03;\n    color: #ffffffba;\n}\n.dark .card_purple {\n    background-color: #370040;\n    color: #ffffffba;\n}\n</style>"}, "/ai/en/deploy/ax-pi.html": {"title": "Deploy models to AX-Pi (Maix-III(M3) series) board", "content": "---\ntitle: Deploy models to AX-Pi (Maix-III(M3) series) board\ndate: 2022-09-21\n---\n\n\nnot translate yet, Chinese see [here](../../zh/deploy/ax-pi.html), translation is welcome!"}, "/ai/en/deploy/v831.html": {"title": "Deploy models to V831", "content": "---\ntitle: Deploy models to V831\ndate: 2022-09-15\n---\n\n>! This document is not translate yet, translation is welcome\n\n## 制作浮点模型\n\n对于 V831， 强烈推荐使用`Pytorch`训练模型，因为模型转换工具对其支持较好。\n\n这里直接使用 pytorch hub 的预训练模型为例。\n\n这里省略了模型定义和训练过程， 直接使用 pytorch hub 的 resnet18 预训练模型进行简单介绍：\nhttps://pytorch.org/hub/pytorch_vision_resnet/\n\n\n注意 V831 支持的算子有限，具体请在 [MaixHub](https://maixhub.com/) 点击`工具箱->模型转换->v831`中的文档查看。\n\n\n## 在 PC 端测试模型推理\n\n\n根据上面链接的使用说明， 使用如下代码可以运行模型\n\n其中， label 下载： https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n```python\nimport os\nimport torch\nfrom torchsummary import summary\n\n\n## model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n\nmodel.eval()\n\ninput_shape = (3, 224, 224)\nsummary(model, input_shape, device=\"cpu\")\n\n## test image\nfilename = \"out/dog.jpg\"\nif not os.path.exists(filename):\n    if not os.path.exists(\"out\"):\n        os.makedirs(\"out\")\n    import urllib\n    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n    try: urllib.URLopener().retrieve(url, filename)\n    except: urllib.request.urlretrieve(url, filename)\n\nprint(\"test image:\", filename)\n\n## preparing input data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\ninput_image = Image.open(filename)\n# input_image.show()\npreprocess = transforms.Compose([\n    transforms.Resize(max(input_shape[1:3])),\n    transforms.CenterCrop(input_shape[1:3]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\n\nprint(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor)))\n\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n## forward model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\n\nwith torch.no_grad():\n    output = model(input_batch)\n\n## result    \n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n# print(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nmax_1000 = torch.nn.functional.softmax(output[0], dim=0)\nmax_idx = int(torch.argmax(max_1000))\nwith open(\"imagenet_classes.txt\") as f:\n    labels = f.read().split(\"\\n\")\nprint(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx]))\n```\n\n运行后 结果：\n```\nUsing cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 44.59\nEstimated Total Size (MB): 107.96\n----------------------------------------------------------------\nout/dog.jpg\ntensor(2.6400) tensor(-2.1008)\nidx:258, name:Samoyed, Samoyede\n```\n\n可以看到模型有 `11,689,512`的参数， 即 `11MiB`左右， 这个大小也就几乎是实际在 831 上运行的模型的大小了\n\n## 将模型转换为 V831 能使用的模型文件\n\n转换过程如下：\n\n### 使用 Pytorch 将模型导出为 `onnx`模型， 得到`onnx`文件\n\n```python\ndef torch_to_onnx(net, input_shape, out_name=\"out/model.onnx\", input_names=[\"input0\"], output_names=[\"output0\"], device=\"cpu\"):\n    batch_size = 1\n    if len(input_shape) == 3:\n        x = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype=torch.float32, requires_grad=True).to(device)\n    elif len(input_shape) == 1:\n        x = torch.randn(batch_size, input_shape[0], dtype=torch.float32, requires_grad=False).to(device)\n    else:\n        raise Exception(\"not support input shape\")\n    print(\"input shape:\", x.shape)\n    # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params=True)\n    torch.onnx.export(net, x, out_name, export_params=True, input_names = input_names, output_names=output_names)\n\n\nonnx_out=\"out/resnet_1000.onnx\"\nncnn_out_param = \"out/resnet_1000.param\"\nncnn_out_bin = \"out/resnet_1000.bin\"\ninput_img = filename\n\ntorch_to_onnx(model, input_shape, onnx_out, device=\"cuda:0\")\n\n```\n\n如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字\n\n## 使用 `onnx2ncnn` 工具将`onnx`转成`ncnn`模型，得到一个`.param`文件和一个`.bin`文件\n\n>! 这一步可以跳过。\n\n> 按照[ncnn项目](https://github.com/Tencent/ncnn)的编译说明编译，在`build/tools/onnx`目录下得到`onnx2ncnn`可执行文件\n\n```python\ndef onnx_to_ncnn(input_shape, onnx=\"out/model.onnx\", ncnn_param=\"out/conv0.param\", ncnn_bin = \"out/conv0.bin\"):\n    import os\n    # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir\n    cmd = f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\"\n    os.system(cmd)\n    with open(ncnn_param) as f:\n        content = f.read().split(\"\\n\")\n        if len(input_shape) == 1:\n            content[2] += \" 0={}\".format(input_shape[0])\n        else:\n            content[2] += \" 0={} 1={} 2={}\".format(input_shape[2], input_shape[1], input_shape[0])\n        content = \"\\n\".join(content)\n    with open(ncnn_param, \"w\") as f:\n        f.write(content)\n\nonnx_to_ncnn(input_shape, onnx=onnx_out, ncnn_param=ncnn_out_param, ncnn_bin=ncnn_out_bin)\n```\n\n## 使用全志提供的`awnn`工具将`ncnn`模型进行量化到`int8`模型\n\n在 [MaixHub](https://maixhub.com/) 点击`工具箱->模型转换->v831`进入模型转换页面， 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做）\n\n在转换页面有更多的转换说明，可以获得更多详细的转换说明\n\n这里有几组参数：\n* 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值  - mean ) / std`, `awnn`对输入的处理是 `(输入值 - mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给`awnn`量化工具经过` (输入值 - mean ) * norm` 计算后的值范围一致既可。 比如 这里打印了实际数据的输入范围是`[-2.1008, 2.6400]`， 是代码中`preprocess` 对象处理后得到的，即`x = (x - mean) / std` ==> `(0-0.485)/0.229 = -2.1179`, 到`awnn`就是`x = (x - mean_2*255) * (1 / std * 255)` 即 `mean2 = mean * 255`, `norm = 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result#pre-process)。\n所以我们这里可以设置 均值为 `0.485 * 255 = 123.675`， 设置 归一化因子为`1/ (0.229 * 255) = 0.017125`， 另外两个通道同理，但是目前 awnn 只能支持三个通道值一样。。。所以填`123.675, 123.675, 123.675`，`0.017125, 0.017125, 0.017125` 即可，因为这里用了`pytorch hub`的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下\n\n* 图片输入层尺寸（问不是图片怎么办？貌似 awnn 暂时只考虑到了图片。。）\n* RGB 格式： 如果训练输入的图片是 RGB 就选 RGB\n* 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手\n\n自己写的其它模型转换如果失败，多半是啥算子不支持，需要在 使用说明里面看支持的 算子，比如之前的版本view、 flatten、reshape  都不支持所以写模型要相当小心， 现在的版本会支持 flatten reshape 等 CPU 算子\n\n如果不出意外， 终于得到了量化好的 awnn 能使用的模型， `*.param` 和 `*.bin`\n\n\n\n## 使用模型，在v831上推理\n\n可以使用 python 或者 C 写代码，以下两种方式\n\n### MaixPy3\n\npython 请看[MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/)\n\n不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了：\n```\npip install --upgrade maixpy3\n```\n\n然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）：\n\nhttps://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py\n\nlabel 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py\n\n```python\nfrom maix import nn\nfrom PIL import Image, ImageDraw\nfrom maix import camera, display\n\ntest_jpg = \"/root/test_input/input.jpg\"\nmodel = {\n    \"param\": \"/root/models/resnet_awnn.param\",\n    \"bin\": \"/root/models/resnet_awnn.bin\"\n}\n\ncamera.config(size=(224, 224))\n\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (1, 1, 1000)\n    },\n    \"first_layer_conv_no_pad\": False,\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n}\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nprint(\"-- read image\")\nimg = Image.open(test_jpg)\nprint(\"-- read image ok\")\nprint(\"-- forward model with image as input\")\nout = m.forward(img, quantize=True)\nprint(\"-- read image ok\")\nprint(\"-- out:\", out.shape)\nout = nn.F.softmax(out)\nprint(out.max(), out.argmax())\n\nfrom classes_label import labels\nwhile 1:\n    img = camera.capture()\n    if not img:\n        time.sleep(0.02)\n        continue\n    out = m.forward(img, quantize=True)\n    out = nn.F.softmax(out)\n    msg = \"{:.2f}: {}\".format(out.max(), labels[out.argmax()])\n    print(msg)\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 0), msg, fill=(255, 0, 0))\n    display.show(img)\n```\n\n\n### C语言 SDK， libmaix\n\n访问这里，按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet\n\n上传编译成功后`dist`目录下的所有内容到 `v831`, 然后执行`./start_app.sh`即可\n\n\n## 参考\n\n* [在V831上（awnn）跑 pytorch resnet18 模型](https://neucrack.com/p/358)"}, "/ai/en/deploy/tinymaix.html": {"title": "Use TinyMaix to deploy AI models to MCU", "content": "---\ntitle: Use TinyMaix to deploy AI models to MCU\ndate: 2022-09-15\n---\n\n\n[TinyMaix](https://github.com/sipeed/TinyMaix) is a model inference runtime designed for limited memory and compute resources MCU, can even run `MNIST` on `2KB` RAM's MCU `Arduino ATmega328`, especially optimized for many architecture like RISC-V, ARM Cortex-M etc.\n\n\nMore details see [TinyMaix official repository](https://github.com/sipeed/TinyMaix)"}, "/ai/en/deploy/k210.html": {"title": "Deploy model to Maix-I(M1) K210 series development boards", "content": "---\ntitle: Deploy model to Maix-I(M1) K210 series development boards\ndate: 2022-09-15\n---\n\n>! This document is not translate yet, translation is welcome\n\n> 欢迎修改和补充\n\n一般使用 `tensorflow` 训练出浮点模型， 再使用转换工具将其转换成 `K210` 所支持的 `Kmodel` 模型，然后将模型部署到 `K210` 开发板上。\n\n\n## K210 上的 KPU\n\n`K210` 上的 AI 硬件加速单元取名为`KPU`，`KPU` 实现了 卷积、批归一化、激活、池化 这 4 种基础操作的硬件加速， 但是它们不能分开单独使用，是一体的加速模块。\n\n所以， 在 KPU 上面推理模型， 以下要求：\n\n### 内存限制\n\n K210 有 6MB 通用 RAM 和 2MB KPU 专用 RAM。模型的输入和输出特征图存储在 2MB KPU RAM 中。权重和其他参数存储在 6MB 通用 RAM 中，在转换模型时，会打印模型使用的内存大小以及临时最大内存使用情况。\n\n### 哪些算子可以被 KPU 完全加速？\n\nnncase 支持的算子：\n  * nncase v0.2.0 支持的算子： https://github.com/kendryte/nncase/blob/master/docs/tflite_ops.md\n  * nncase v0.1.0 支持的算子： https://github.com/kendryte/nncase/tree/v0.1.0-rc5\n\n下面的约束需要全部满足。\n\n  * 特征图尺寸：输入特征图小于等于 320x240 (宽x高) 同时输出特征图大于等于 4x4 (宽x高)，通道数在 1 到 1024。\n  * Same 对称 paddings (TensorFlow 在 stride=2 同时尺寸为偶数时使用非对称 paddings)。\n  * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，stride 为 1 或 2。\n  * 最大池化 MaxPool (2x2 或 4x4) 和 平均池化 AveragePool (2x2 或 4x4)。\n  * 任意逐元素激活函数 (ReLU, ReLU6, LeakyRelu, Sigmoid...), KPU 不支持 PReLU。\n\n### 哪些算子可以被 KPU 部分加速？\n\n  * 非对称 paddings 或 valid paddings 卷积， nncase 会在其前后添加必要的 Pad 和 Crop（可理解为 边框 与 裁切）。\n  * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，但 stride 不是 1 或 2。 nncase 会把它分解为 KPUConv2D 和一个 StridedSlice (可能还需要 Pad)。\n  * MatMul 算子， nncase 会把它替换为一个 Pad(到 4x4)+ KPUConv2D(1x1 卷积和) + Crop(到 1x1)。\n  * TransposeConv2D 算子， nncase 会把它替换为一个 SpaceToBatch + KPUConv2D + BatchToSpace。\n\n> 以上说明来自[这里](https://github.com/kendryte/nncase/blob/master/docs/FAQ_ZH.md)\n\n\n## 训练出浮点模型\n\n对于 K210， 建议使用 TensorFlow，因为它的转换工具对其支持最好。\n\ntensorflow 举个例子， 两分类模型， 这里是随便叠的层结构\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ninput_shape = (240, 320, 3)\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(layers.ZeroPadding2D(input_shape = input_shape, padding=((1, 1), (1, 1))))\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid', strides = (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); #model.add(MaxPool2D());\n\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(64, (3,3), padding = 'valid',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(64, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(64, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2))\nmodel.add(layers.Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(\n              loss ='sparse_categorical_crossentropy',\n              optimizer = 'adam',\n              metrics =['accuracy'])\n\nmode.fit(...)\n```\n\n这里你可能注意到了, 在 `conv` 层中`stride != 1` 时, 都加了一个 `zeropadding` 层, 这是 K210 硬件支持的模式, 如果不这样做, 转换成 V3 模型时(使用 nncase v0.1.0 RC5) 则直接报错, 使用 V4 模型(nncase V0.2.0转换)可以通过,但是是使用软件运算的, 会消耗大量内存和时间, 会发现内存占用大了很多!!! 所以设计模型时也需要注意\n\n\n## 转换工具\n\n使用 K210 芯片官方提供的 [nncase](https://github.com/kendryte/nncase) 工具来进行转换。\n\n需要注意的是，工具版本更新迭代比较多， `K210`属于第一代芯片，算子支持有限，并且内存只有`6MiB（通用）+2MiB（AI专用）`内存，所以最新版本的工具可能并不是最好的选择，根据需求选择合适的版本。\n\n由于代码更新， 在过程中**模型格式**产生了两个大版本， `V3` 和 `V4`， 其中 `V3` 模型是指用 [nncase v0.1.0 RC5](https://github.com/kendryte/nncase/releases/tag/v0.1.0-rc5) 转换出来的模型； `V4`模型指用 [nncase v0.2.0](https://github.com/kendryte/nncase/releases/tag/v0.2.0-beta4) 转换出来的模型，以及 V5 或更新版本等等。\n\n两者有一定的不同，所以现在两者共存， `V3` 代码量更少，占用内存小，效率也高，但是支持的算子少； `V4` 支持的算子更多，但是都是软件实现的，没有硬件加速，内存使用更多，所以各有所长。 `MaixPy` 的固件也可以选择是否支持 `V4`， 如果你的模型 `V3` 能够满足算子支持，强烈建议用 `V3`，在遇到算子不支持而且一定要用那个算子时再用`V4`。\n\n\n\n## 运行测试模型\n\n使用 [MaixPy](/maixpy) 来运行模型，也可以用 [C SDK](https://github.com/sipeed/LicheeDan_K210_examples) 写。\n\n\n比如使用 `MaixPy`固件， 将模型放到 SD 卡， 然后使用代码加载\n\n ```python\n    import KPU as kpu\n    import image\n    m = kpu.load(\"/sd/test.kmodel\")\n    img = image.Image(\"/sd/test.jpg\")\n    img = img.resize(224, 224)\n    img.pix_to_ai()\n    feature_map = kpu.forward(m, img)\n    p_list = feature_map[:]\n ```\n\n## 更多参考\n\n* [K210 MaixPy 从入门到飞升--AI视觉篇--完全教程（以及一些小问题处理比如内存不足）](https://neucrack.com/p/325)\n* [MaixPy AI 硬件加速基本知识](/soft/maixpy/zh/course/ai/basic/maixpy_hardware_ai_basic.html)\n\n\n## 上传分享到 MaixHub\n\n可以上传分享你的模型到到 [MaixHub](https://maixhub.com/) 的模型库，可以让更多人发现并使用你的模型~ 一起做出更多有趣的项目吧！（K210 模型支持加密分享）\n\n另外你也可以使用 [MaixHub](https://maixhub.com/) 的模型库，下载别人分享的模型，或者使用在线训练出的模型，直接使用或者参考模型结构。"}, "/ai/en/basic/what_is_ai.html": {"title": "What is AI and machine learning?", "content": "---\ntitle: What is AI and machine learning?\ndate: 2022-09-15\n---\n\n>! This document is not translate yet, translation is welcome\n\nAI（Artificial Intelligence） 想必大家都听之极多了，有人觉得 AI 要毁灭人类了；有人认为 AI 只是擅长某些特殊场景不必惊慌（比如 alpha Go 打败了人类围棋大师）；但大多数人接触到的 AI 可能更愿意称之为“人工智障”，比如手机里面的助手；对于很多工程师来说， AI 可能更多的是指机器学习，比如图像识别，语音识别，自然语言处理等等。\n\n作为一个开发者， 首先我们需要了解大家经常听到的 `AI`，`机器学习`，`神经网络`等概念以及区别：\n* AI： 人工智能，是指让机器具有人类智能的能力，比如人类可以看到一张照片，然后判断出这是一只猫，这是一个人，这是一只狗等等，而机器也可以做到这一点，这就是人工智能。\n* 机器学习： 一般是指让机器通过大量的数据，然后通过算法，让机器自己学习，比如通过大量的猫的图片，让机器自己学习，然后判断出一张图片是不是猫。\n* 神经网络： 一般是指在机器学习中用到的一种数据结构，因为其类似人大脑的神经网络，各个数据节点互相连接互相通信和影响，故称之为神经网络。\n* 模型： 指用来承载和表示机器学习过程中的相关参数的数据结构，一般可以保存为一个文件，可以将神经网络的结构和参数都存储在这个数据结构里面，方便用数学或者编程语言将其解析，比如取名叫`.model`格式的文件\n\n所以可以理解为三者是包含关系： 神经网络 ∈ 机器学习 ∈ 人工智能， 模型文件则一般为机器学习中的产物， 另外你可能还听说过“深度神经网络”，其实也是属于神经网络，只不过是网络层数有深度不同一说。\n\n而本文也大多阐述了如何利用各种神经网络模型和机器学习的方法来实现 AI 应用。\n\n## 机器学习过程简介\n\n这里首先对机器学习的过程做一个通俗的介绍，不涉及数学公式，只是简单的介绍一下机器学习的过程。\n\n### 训练\n\n以让机器区分猫咪和狗为例：\n和教（训练）人类婴儿一样，可以把模型比作婴儿，为了让这个模型能认识猫和狗，我们需要一遍一遍地让它看各种猫猫狗狗，并让它去识别，错了我们就告诉它错了，对了我们就告诉它对了，这样一遍一遍地让它看，让它学习，最终它就能区分猫和狗了。\n\n从这段话我们分析出训练的时候几个关键点：\n* 模型： 一个工具或者黑盒，给它一个输入，它能给我们一个输出结果。\n* 输入： 这里是图像，猫或者狗的图像。\n* 输出： 猫或者狗\n* 判断错误的方法：也就是它的输出和真实的结果是否一样，这里是靠教学者判断正误的，也就是判断错误的方法是教学者。\n* 学习方法：就是当我们告诉模型输出结果是错的时候，它如何去改进。\n\n得到这几个关键点后，就可以很好地理解这个机器学习的过程了：\n* 定义模型的输入输出， 输入是图像，输出是猫或者狗。\n* 为了让这个模型能够有学习的本领，也就是和人一样有足够的脑容量， 我们定义一个属于模型的“脑子”，也就是一个看起来和人脑突触结构类似的神经网络结构：\n![神经网络](../../assets/dnn.jpg)\n可以看到输入和中间每个节点间都有线连起来，每条线都是一个计算公式，具体是什么样的公式以及具体如何设计一个这样的结构这里先不细究，先有个概念就行。\n* 然后就是判断错误和误差的方法，一般在代码中称之为损失函数，也就是模型的输出结果是否正确。\n* 然后就是学习方法，比如结果不正确，如何去微调模型内部的参数，让下一次的输出结果更接近正确的结果。\n\n### 验证\n\n经过很多数据的反复训练后，我们发现好像基本都能识别正确了，但是我们还是担心我们用的图片种类是否不够多，这个模型是否真的能够识别所有的猫和狗或者其它猫和狗，这个时候就需要验证了，验证的方法就是把模型拿出来，给它一些新的数据，即在训练的时候从来没用到过的数据，让它去识别，看看它的识别结果是否正确，如果正确，那我们就认为这个模型泛化效果不错，可以放心的使用这个模型去识别毛毛狗狗了。\n\n一般我们会在训练是一段时间后拿出`验证集`（也就是用来验证的数据集，对应训练的数据叫`训练集`）来测试一下模型的效果，如果效果不错就可以停止训练了，如果效果不好，则需要继续训练或者考虑是不是训练集有问题，或者模型结构、损失函数、学习方法等有问题了。\n\n\n### 测试\n\n验证效果的好坏决定了我们合适停止训练，也就是说模型效果如何和`验证集`紧密相关，相当于**验证集也变相地参与了模型的训练过程**， 所以在结束训练后，我们用一个新的数据集`测试集`来测试一下模型的效果，这个数据集是在训练和验证的时候从来没用过的，这样就可以更加客观地评估模型的效果了。\n\n这里共提到了`训练集`，`验证集`，`测试集`，需要注意他们三个数据集的区别！前两者在训练过程参与，后者在训练过程不参与，只是用来评估模型的效果，并且三者互相不重合，防止训练过程中模型只对一小部分数据有效，到了新的场景就无法识别（也就是所谓的`过拟合`）。\n\n### 总结\n\n这里简单阐述了机器学习的过程的通俗解释，你也可以到 [MaixHub](https://maixhub.com) 注册登录后体验自己体验一遍在线训练过程加深理解，无需懂代码，懂得这里描述的机器学习的过程就可以了，然后再进行进一步学习。"}, "/ai/en/maixhub/index.html": {"title": "Introduction to MaixHub", "content": "---\ntitle: Introduction to MaixHub\n---\n\n[MaixHub](https://maixhub.com) is a platform released by Sipeed that integrates functions such as AI model service and community communication. It mainly provides the following functions:\n* Model library, directly download the model to the device to run and use, and share your own model to the model library.\n* Online training, you can easily train models without programming foundation and AI training experience, which is convenient for getting started with AI learning and accelerating AI application development.\n* Project sharing, share your own projects and works, exchange learning or find inspiration in the community.\n\nFor more functions and more content, please visit [MaixHub](https://maixhub.com)."}, "/ai/en/maixhub/train_best.html": {"title": "Optimize MaixHub online train", "content": "---\ntitle: Optimize MaixHub online train\n---\n\n>! this document need translation, help is welcome\n\nWhen using MaixHub to train a model, the recognition effect may not be very good or the actual running speed of the model may be slow. Here are some common tuning methods.\n\nModifications and additions are welcome\n\n\n## 识别效果优化\n\n* 尽量多采集实际使用场景的图片，覆盖更多使用场景有利于提高最终识别率。\n\n* 图片数量尽量不要太小，虽然平台限制最小数量为 20 张图才可以训练， 但要达到比较好的效果，显然一个分类 200 张都不算多，不要一直在 30 张训练图片上纠结为什么训练效果不好。。。\n\n* 修改迭代次数，在发现`val_acc`仍然有上升趋势的情况下可以考虑适当增大迭代次数，但是迭代次数越大，训练时间越长，所以要根据实际情况权衡。\n\n* 修改学习率和批数量大小，学习率不宜太大，否则会导致梯度爆炸出现`loss 为 0`或者`loss 为 inf`这样的错误，批数量大小不宜太小，否则会导致训练速度过慢，一般来说，学习率在 0.0001~0.001 之间，批数量大小在 16~64 之间都是比较合适的。另外需要注意批数量越大， 学习率就可以设置得稍微大一点。\n\n* 每个标签的数据量都尽量多，而不是一个标签只有 20张，另一个500张图， 可以把训练参数处的“数据均衡“开关打开\n\n* 默认分辨率但是 224x224， 是因为预训练模型是在 224x224 下训练的，当然也有其它分辨率的，比如 128x128，具体发现不支持的分辨率预训练模型，在训练日志中会打印警告信息。\n\n* 为了让验证集的精确度的可信度更高（也就是在实际开发板上跑的精确度更接近训练时在验证集上的精确度），验证集的数据和实际应用的场景数据一致。比如训练集是在网上找了很多图片，那这些图片可能和实际开发板的摄像头拍出来的图有差距，可以往验证集上传一些实际设备拍的图来验证训练的模型效果。\n  这样我们就能在训练的时候根据验证集精确度（val_acc）来判断模型训练效果如何了，如果发现验证集精确度很低，那么就可以考虑增加训练集复杂度和数量，或者训练集用设备拍摄来训练。\n\n* 对于检测训练项目，如果检测训练的物体很准，但是容易误识别到其它物体，可以在数据里面拍点其它的物体当背景；或者拍摄一些没有目标的图片，不添加任何标注也可以，然后在训练的时候勾选**允许负样本**来使能没有标注的图片。\n\n* 检测任务可以同时检测到多个目标，如果你觉得识别类别不准，也有另外一种方式，先只检测模型检测到物体（一个类别），然后裁切出图片中的目标物体上传到分类任务，用分类任务来分辨类别。不过这样就要跑两个模型，需要写代码裁切图片（在板子跑就好了），以及需要考虑内存是否足够\n\n\n## 在开发板上运行速度慢\n\n* 减小输入分辨率，比如在分类任务中可以使用`96x96`的小图来训练。\n* 裁切一部分图像进行识别，识别时不对整张图片进行识别，可以裁切出部分图像进行识别。\n* 选择更小的网络，比如分类选择`mobilenetV1 0.25`是`219KiB`, 而`mobilenetV1 0.75`则是`1.85MiB`，网络参数量减少了很多，不过相应地，模型识别精度也会降低。\n\n\n## 更快地标注数据\n\n* 可以导入本地已经标注好了的数据到 MaixHub。\n* MaixHub 支持视频辅助自动标注，只需拍摄视频上传的时候使用辅助标注功能即可，对于画面中只有单个物体的场景标注十分有用。\n* 可以使用已经训练好的模型来辅助标注，虽然现在 MaixHub 不支持用训练好的模型来标注，但是你可以下载训练好模型到板子运行，写代码将识别到的物体坐标保存为`VOC`标注格式，就得到了新的标注数据，虽然可能会因为模型训练效果不够好标得不够准确，但是经过简单的手工筛选调整后，标注数据就可以新的训练了，如此反复，就会得到很多数据啦。\n* MaixHub 未来可能会上线更好用的辅助标注工具哦~ 有建议欢迎通过 MaixHub 的反馈功能告诉我们哦~"}, "/ai/en/no_translate.html": {"title": "no translation", "content": "---\ntitle: no translation\nclass: md_page\n---\n\n\n<div id=\"visit_from\"></div>\n<div id=\"no_translate_hint\">This page not translated yet</div>\n<div>\n    <span id=\"visit_hint\">Please visit</span>\n    <a id=\"translate_src\"></a>\n</div>\n\n<div>\n    <script>\n        function getQueryVariable(variable)\n        {\n            var query = window.location.search.substring(1);\n            var vars = query.split(\"&\");\n            for (var i=0;i<vars.length;i++) {\n                    var pair = vars[i].split(\"=\");\n                    if(pair[0] == variable){return pair[1];}\n            }\n            return(false);\n        }\n        var ref = getQueryVariable(\"ref\");\n        var from = getQueryVariable(\"from\");\n        var link = document.getElementById(\"translate_src\");\n        var fromDis = document.getElementById(\"visit_from\");\n        link.href = ref;\n        link.text = ref;\n        fromDis.innerHTML = from;\n    </script>\n</div>"}}