{"/ai/zh/deploy/k210.html": {"title": "部署模型到 Maix-I(M1) K210 系列开发板", "content": "---\ntitle: 部署模型到 Maix-I(M1) K210 系列开发板\ndate: 2022-09-15\n---\n\n<div id=\"title_card\">\n    <div class=\"card\">\n        <img src=\"/hardware/zh/maix/assets/dk_board/maix_duino/maixduino_0.png\" alt=\"K210 模型转换和部署\">\n        <div class=\"card_info card_red\">\n            <div class=\"title\">Maix-I 系列 K210</div>\n            <div class=\"brief\">\n                <div>高性价比带硬件 AI 加速的单片机</div>\n                <div>1Tops@INT8，有限算子加速</div>\n            </div>\n        </div>\n    </div>\n</div>\n<style>\n#title_card {\n    width:100%;\n    text-align:center;\n    margin-bottom: 1em;\n}\n#title_card img {\n  max-height: 20em;\n}\n.card_red {\n    background-color: #ffcdd2;\n    color: #cf4f5a;\n}\n.dark .card_red {\n    background-color: #5a0000;\n    color: #ffffffba;\n}\n.title {\n    font-size: 1.5em;\n    font-weight: 800;\n    padding: 0.8em;\n}\n</style>\n\n> 欢迎修改和补充\n\n一般使用 `tensorflow` 训练出浮点模型， 再使用转换工具将其转换成 `K210` 所支持的 `Kmodel` 模型，然后将模型部署到 `K210` 开发板上。\n\n\n## K210 上的 KPU\n\n`K210` 上的 AI 硬件加速单元取名为`KPU`，`KPU` 实现了 卷积、批归一化、激活、池化 这 4 种基础操作的硬件加速， 但是它们不能分开单独使用，是一体的加速模块。\n\n所以， 在 KPU 上面推理模型， 以下要求：\n\n### 内存限制\n\n K210 有 6MB 通用 RAM 和 2MB KPU 专用 RAM。模型的输入和输出特征图存储在 2MB KPU RAM 中。权重和其他参数存储在 6MB 通用 RAM 中，在转换模型时，会打印模型使用的内存大小以及临时最大内存使用情况。\n\n### 哪些算子可以被 KPU 完全加速？\n\nnncase 支持的算子：\n  * nncase v0.2.0 支持的算子： https://github.com/kendryte/nncase/blob/master/docs/tflite_ops.md\n  * nncase v0.1.0 支持的算子： https://github.com/kendryte/nncase/tree/v0.1.0-rc5\n\n下面的约束需要全部满足。\n\n  * 特征图尺寸：输入特征图小于等于 320x240 (宽x高) 同时输出特征图大于等于 4x4 (宽x高)，通道数在 1 到 1024。\n  * Same 对称 paddings (TensorFlow 在 stride=2 同时尺寸为偶数时使用非对称 paddings)。\n  * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，stride 为 1 或 2。\n  * 最大池化 MaxPool (2x2 或 4x4) 和 平均池化 AveragePool (2x2 或 4x4)。\n  * 任意逐元素激活函数 (ReLU, ReLU6, LeakyRelu, Sigmoid...), KPU 不支持 PReLU。\n\n### 哪些算子可以被 KPU 部分加速？\n\n  * 非对称 paddings 或 valid paddings 卷积， nncase 会在其前后添加必要的 Pad 和 Crop（可理解为 边框 与 裁切）。\n  * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，但 stride 不是 1 或 2。 nncase 会把它分解为 KPUConv2D 和一个 StridedSlice (可能还需要 Pad)。\n  * MatMul 算子， nncase 会把它替换为一个 Pad(到 4x4)+ KPUConv2D(1x1 卷积和) + Crop(到 1x1)。\n  * TransposeConv2D 算子， nncase 会把它替换为一个 SpaceToBatch + KPUConv2D + BatchToSpace。\n\n> 以上说明来自[这里](https://github.com/kendryte/nncase/blob/master/docs/FAQ_ZH.md)\n\n\n## 训练出浮点模型\n\n对于 K210， 建议使用 TensorFlow，因为它的转换工具对其支持最好。\n\ntensorflow 举个例子， 两分类模型， 这里是随便叠的层结构\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ninput_shape = (240, 320, 3)\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(layers.ZeroPadding2D(input_shape = input_shape, padding=((1, 1), (1, 1))))\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid', strides = (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); #model.add(MaxPool2D());\n\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'valid',strides = (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(32, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.ZeroPadding2D(padding=((1, 1), (1, 1))));\nmodel.add(layers.Conv2D(64, (3,3), padding = 'valid',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\nmodel.add(layers.Conv2D(64, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\nmodel.add(layers.Conv2D(64, (3,3), padding = 'same',strides = (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu'));\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2))\nmodel.add(layers.Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(\n              loss ='sparse_categorical_crossentropy',\n              optimizer = 'adam',\n              metrics =['accuracy'])\n\nmode.fit(...)\n```\n\n这里你可能注意到了, 在 `conv` 层中`stride != 1` 时, 都加了一个 `zeropadding` 层, 这是 K210 硬件支持的模式, 如果不这样做, 转换成 V3 模型时(使用 nncase v0.1.0 RC5) 则直接报错, 使用 V4 模型(nncase V0.2.0转换)可以通过,但是是使用软件运算的, 会消耗大量内存和时间, 会发现内存占用大了很多!!! 所以设计模型时也需要注意\n\n\n## 转换工具\n\n使用 K210 芯片官方提供的 [nncase](https://github.com/kendryte/nncase) 工具来进行转换。\n\n需要注意的是，工具版本更新迭代比较多， `K210`属于第一代芯片，算子支持有限，并且内存只有`6MiB（通用）+2MiB（AI专用）`内存，所以最新版本的工具可能并不是最好的选择，根据需求选择合适的版本。\n\n由于代码更新， 在过程中**模型格式**产生了两个大版本， `V3` 和 `V4`， 其中 `V3` 模型是指用 [nncase v0.1.0 RC5](https://github.com/kendryte/nncase/releases/tag/v0.1.0-rc5) 转换出来的模型； `V4`模型指用 [nncase v0.2.0](https://github.com/kendryte/nncase/releases/tag/v0.2.0-beta4) 转换出来的模型，以及 V5 或更新版本等等。\n\n两者有一定的不同，所以现在两者共存， `V3` 代码量更少，占用内存小，效率也高，但是支持的算子少； `V4` 支持的算子更多，但是都是软件实现的，没有硬件加速，内存使用更多，所以各有所长。 `MaixPy` 的固件也可以选择是否支持 `V4`， 如果你的模型 `V3` 能够满足算子支持，强烈建议用 `V3`，在遇到算子不支持而且一定要用那个算子时再用`V4`。\n\n\n\n## 运行测试模型\n\n使用 [MaixPy](/maixpy) 来运行模型，也可以用 [C SDK](https://github.com/sipeed/LicheeDan_K210_examples) 写。\n\n\n比如使用 `MaixPy`固件， 将模型放到 SD 卡， 然后使用代码加载\n\n ```python\n    import KPU as kpu\n    import image\n    m = kpu.load(\"/sd/test.kmodel\")\n    img = image.Image(\"/sd/test.jpg\")\n    img = img.resize(224, 224)\n    img.pix_to_ai()\n    feature_map = kpu.forward(m, img)\n    p_list = feature_map[:]\n ```\n\n## 更多参考\n\n* [K210 MaixPy 从入门到飞升--AI视觉篇--完全教程（以及一些小问题处理比如内存不足）](https://neucrack.com/p/325)\n* [MaixPy AI 硬件加速基本知识](/soft/maixpy/zh/course/ai/basic/maixpy_hardware_ai_basic.html)\n\n\n## 上传分享到 MaixHub\n\n可以上传分享你的模型到到 [MaixHub](https://maixhub.com/) 的模型库，可以让更多人发现并使用你的模型~ 一起做出更多有趣的项目吧！（K210 模型支持加密分享）\n\n另外你也可以使用 [MaixHub](https://maixhub.com/) 的模型库，下载别人分享的模型，或者使用在线训练出的模型，直接使用或者参考模型结构。"}, "/ai/zh/basic/what_is_ai.html": {"title": "什么是人工智能(AI)和机器学习", "content": "---\ntitle: 什么是人工智能(AI)和机器学习\ndate: 2022-09-15\n---\n\nAI（Artificial Intelligence） 想必大家都听之极多了，有人觉得 AI 要毁灭人类了；有人认为 AI 只是擅长某些特殊场景不必惊慌（比如 alpha Go 打败了人类围棋大师）；但大多数人接触到的 AI 可能更愿意称之为“人工智障”，比如手机里面的助手；对于很多工程师来说， AI 可能更多的是指机器学习，比如图像识别，语音识别，自然语言处理等等。\n\n作为一个开发者， 首先我们需要了解大家经常听到的 `AI`，`机器学习`，`神经网络`等概念以及区别：\n* AI： 人工智能，是指让机器具有人类智能的能力，比如人类可以看到一张照片，然后判断出这是一只猫，这是一个人，这是一只狗等等，而机器也可以做到这一点，这就是人工智能。\n* 机器学习： 一般是指让机器通过大量的数据，然后通过算法，让机器自己学习，比如通过大量的猫的图片，让机器自己学习，然后判断出一张图片是不是猫。\n* 神经网络： 一般是指在机器学习中用到的一种数据结构，因为其类似人大脑的神经网络，各个数据节点互相连接互相通信和影响，故称之为神经网络。\n* 模型： 指用来承载和表示机器学习过程中的相关参数的数据结构，一般可以保存为一个文件，可以将神经网络的结构和参数都存储在这个数据结构里面，方便用数学或者编程语言将其解析，比如取名叫`.model`格式的文件\n\n所以可以理解为三者是包含关系： 神经网络 ∈ 机器学习 ∈ 人工智能， 模型文件则一般为机器学习中的产物， 另外你可能还听说过“深度神经网络”，其实也是属于神经网络，只不过是网络层数有深度不同一说。\n\n而本文也大多阐述了如何利用各种神经网络模型和机器学习的方法来实现 AI 应用。\n\n## 机器学习过程简介\n\n这里首先对机器学习的过程做一个通俗的介绍，不涉及数学公式，只是简单的介绍一下机器学习的过程。\n\n### 训练\n\n以让机器区分猫咪和狗为例：\n和教（训练）人类婴儿一样，可以把模型比作婴儿，为了让这个模型能认识猫和狗，我们需要一遍一遍地让它看各种猫猫狗狗，并让它去识别，错了我们就告诉它错了，对了我们就告诉它对了，这样一遍一遍地让它看，让它学习，最终它就能区分猫和狗了。\n\n从这段话我们分析出训练的时候几个关键点：\n* 模型： 一个工具或者黑盒，给它一个输入，它能给我们一个输出结果。\n* 输入： 这里是图像，猫或者狗的图像。\n* 输出： 猫或者狗\n* 判断错误的方法：也就是它的输出和真实的结果是否一样，这里是靠教学者判断正误的，也就是判断错误的方法是教学者。\n* 学习方法：就是当我们告诉模型输出结果是错的时候，它如何去改进。\n\n得到这几个关键点后，就可以很好地理解这个机器学习的过程了：\n* 定义模型的输入输出， 输入是图像，输出是猫或者狗。\n* 为了让这个模型能够有学习的本领，也就是和人一样有足够的脑容量， 我们定义一个属于模型的“脑子”，也就是一个看起来和人脑突触结构类似的神经网络结构：\n![神经网络](../../assets/dnn.jpg)\n可以看到输入和中间每个节点间都有线连起来，每条线都是一个计算公式，具体是什么样的公式以及具体如何设计一个这样的结构这里先不细究，先有个概念就行。\n* 然后就是判断错误和误差的方法，一般在代码中称之为损失函数，也就是模型的输出结果是否正确。\n* 然后就是学习方法，比如结果不正确，如何去微调模型内部的参数，让下一次的输出结果更接近正确的结果。\n\n### 验证\n\n经过很多数据的反复训练后，我们发现好像基本都能识别正确了，但是我们还是担心我们用的图片种类是否不够多，这个模型是否真的能够识别所有的猫和狗或者其它猫和狗，这个时候就需要验证了，验证的方法就是把模型拿出来，给它一些新的数据，即在训练的时候从来没用到过的数据，让它去识别，看看它的识别结果是否正确，如果正确，那我们就认为这个模型泛化效果不错，可以放心的使用这个模型去识别毛毛狗狗了。\n\n一般我们会在训练是一段时间后拿出`验证集`（也就是用来验证的数据集，对应训练的数据叫`训练集`）来测试一下模型的效果，如果效果不错就可以停止训练了，如果效果不好，则需要继续训练或者考虑是不是训练集有问题，或者模型结构、损失函数、学习方法等有问题了。\n\n\n### 测试\n\n验证效果的好坏决定了我们合适停止训练，也就是说模型效果如何和`验证集`紧密相关，相当于**验证集也变相地参与了模型的训练过程**， 所以在结束训练后，我们用一个新的数据集`测试集`来测试一下模型的效果，这个数据集是在训练和验证的时候从来没用过的，这样就可以更加客观地评估模型的效果了。\n\n这里共提到了`训练集`，`验证集`，`测试集`，需要注意他们三个数据集的区别！前两者在训练过程参与，后者在训练过程不参与，只是用来评估模型的效果，并且三者互相不重合，防止训练过程中模型只对一小部分数据有效，到了新的场景就无法识别（也就是所谓的`过拟合`）。\n\n### 总结\n\n这里简单阐述了机器学习的过程的通俗解释，你也可以到 [MaixHub](https://maixhub.com) 注册登录后体验自己体验一遍在线训练过程加深理解，无需懂代码，懂得这里描述的机器学习的过程就可以了，然后再进行进一步学习。"}, "/ai/zh/maixhub/index.html": {"title": "MaixHub 简介", "content": "---\ntitle: MaixHub 简介\n---\n\n[MaixHub](https://maixhub.com) 是 Sipeed 发布的集 AI 模型服务和社区沟通等功能的平台，主要提供了以下功能：\n* 模型库，直接下载模型到设备即可运行使用，以及分享自己的模型到模型库。\n* 在线训练，无需编程基础和 AI 训练经验也能轻松训练模型，方便入门 AI 学习和加速 AI 应用开发。\n* 项目分享，分享自己制作的项目和作品，在社区中交流学习或者寻找灵感。\n\n更多功能更多内容，请访问[MaixHub](https://maixhub.com)。"}, "/ai/zh/maixhub/train_best.html": {"title": "MaixHub 训练调优方法", "content": "---\ntitle: MaixHub 训练调优方法\n---\n\n\n在使用 MaixHub 训练模型时，可能会遇到识别效果不太好或者模型实际运行速度慢等情况，此处提供一些常见调优方法。\n\n欢迎修改和补充\n\n\n## 识别效果优化\n\n* 尽量多采集实际使用场景的图片，覆盖更多使用场景有利于提高最终识别率。\n\n* 图片数量尽量不要太小，虽然平台限制最小数量为 20 张图才可以训练， 但要达到比较好的效果，显然一个分类 200 张都不算多，不要一直在 30 张训练图片上纠结为什么训练效果不好。。。\n\n* 修改迭代次数，在发现`val_acc`仍然有上升趋势的情况下可以考虑适当增大迭代次数，但是迭代次数越大，训练时间越长，所以要根据实际情况权衡。\n\n* 修改学习率和批数量大小，学习率不宜太大，否则会导致梯度爆炸出现`loss 为 0`或者`loss 为 inf`这样的错误，批数量大小不宜太小，否则会导致训练速度过慢，一般来说，学习率在 0.0001~0.001 之间，批数量大小在 16~64 之间都是比较合适的。另外需要注意批数量越大， 学习率就可以设置得稍微大一点。\n\n* 每个标签的数据量都尽量多，而不是一个标签只有 20张，另一个500张图， 可以把训练参数处的“数据均衡“开关打开\n\n* 默认分辨率但是 224x224， 是因为预训练模型是在 224x224 下训练的，当然也有其它分辨率的，比如 128x128，具体发现不支持的分辨率预训练模型，在训练日志中会打印警告信息。\n\n* 为了让验证集的精确度的可信度更高（也就是在实际开发板上跑的精确度更接近训练时在验证集上的精确度），验证集的数据和实际应用的场景数据一致。比如训练集是在网上找了很多图片，那这些图片可能和实际开发板的摄像头拍出来的图有差距，可以往验证集上传一些实际设备拍的图来验证训练的模型效果。\n  这样我们就能在训练的时候根据验证集精确度（val_acc）来判断模型训练效果如何了，如果发现验证集精确度很低，那么就可以考虑增加训练集复杂度和数量，或者训练集用设备拍摄来训练。\n\n* 对于检测训练项目，如果检测训练的物体很准，但是容易误识别到其它物体，可以在数据里面拍点其它的物体当背景；或者拍摄一些没有目标的图片，不添加任何标注也可以，然后在训练的时候勾选**允许负样本**来使能没有标注的图片。\n\n* 检测任务可以同时检测到多个目标，如果你觉得识别类别不准，也有另外一种方式，先只检测模型检测到物体（一个类别），然后裁切出图片中的目标物体上传到分类任务，用分类任务来分辨类别。不过这样就要跑两个模型，需要写代码裁切图片（在板子跑就好了），以及需要考虑内存是否足够\n\n\n## 在开发板上运行速度慢\n\n* 减小输入分辨率，比如在分类任务中可以使用`96x96`的小图来训练。\n* 裁切一部分图像进行识别，识别时不对整张图片进行识别，可以裁切出部分图像进行识别。\n* 选择更小的网络，比如分类选择`mobilenetV1 0.25`是`219KiB`, 而`mobilenetV1 0.75`则是`1.85MiB`，网络参数量减少了很多，不过相应地，模型识别精度也会降低。\n\n\n## 更快地标注数据\n\n* 可以导入本地已经标注好了的数据到 MaixHub。\n* MaixHub 支持视频辅助自动标注，只需拍摄视频上传的时候使用辅助标注功能即可，对于画面中只有单个物体的场景标注十分有用。\n* 可以使用已经训练好的模型来辅助标注，虽然现在 MaixHub 不支持用训练好的模型来标注，但是你可以下载训练好模型到板子运行，写代码将识别到的物体坐标保存为`VOC`标注格式，就得到了新的标注数据，虽然可能会因为模型训练效果不够好标得不够准确，但是经过简单的手工筛选调整后，标注数据就可以新的训练了，如此反复，就会得到很多数据啦。\n* MaixHub 未来可能会上线更好用的辅助标注工具哦~ 有建议欢迎通过 MaixHub 的反馈功能告诉我们哦~"}, "/ai/zh/index.html": {"title": "AI 指南", "content": "---\ntitle: AI 指南\ndate: 2022-09-15\n---\n\n\n## 关于本文档\n\n`Sipeed` 推出了一系列 `AI` 开发板，包括:\n* `Maix-I(M1)` 系列的 `MaixBit`，`Maixduino` `M1s-Dock`等带 `AI` 硬件加速的单片机开发板。\n* `Maix-II(M2)` 系列的 `M2-Dock` `MaixSense`等带 `AI`硬件加速的高性价比`SOC`开发板。\n* `Maix-III(M3)`系列的`M3 AX-Pi`等高性能`SOC`开发板。\n\n为了普及 `AI` 在端侧设备(/边缘设备)上的应用，`Sipeed` 开发了易上手的 `MaixPy`和`MaixPy3` SDK, 以及上线了无需编程基础和 AI 训练经验也能轻松训练模型的 [MaixHub](https://maixhub.com/) 平台。\n\n在此基础上，本文档致力于为开发者提供一份 `AI` 开发指南，目的是为了让新手快速上手 `AI` 应用， 或者已经掌握 `AI`相关知识的开发者将成果快速应用到设备或者产品上。\n内容包括不限于:\n* AI 的基础知识\n* 模型训练指南\n* AI 教程推荐\n* AI 有趣项目推荐\n* 常用工具使用\n* 边缘设备部署指南\n\n\n## 参与贡献\n\n文档内容会持续更新，欢迎大家参与内容编写，参与方式：\n\n* 可以直接点击文档右上角的 `编辑本页` 按钮跳转到`GitHub`上进行编辑并提交 `PR`（具体方法可以看[贡献文档](/share_docs/zh/)或者搜索引擎搜`GitHub 如何 提交 PR`）。\n* 也可以直接发送更改建议或者投稿到`support@sipeed.com`, 标题请以`[WiKi 投稿]`开头， 正文需要注明作者，以及需要修改的页面位置和内容，方便我们快速将您的内容更新到文档。"}, "/ai/zh/deploy/index.html": {"title": "制作可部署到边缘设备的模型和部署方法汇总", "content": "---\ntitle: 制作可部署到边缘设备的模型和部署方法汇总\nkeywords: 模型部署, 模型转换, 模型部署边缘设备\ndesc: 汇总如何部署模型到边缘设备的相关文档，边缘设备包含单片机、SBC、SOC、NPU等。\ndate: false\nclass: heading_no_counter\n---\n\n<div id=\"maixhub\">\n    到<a href=\"https://maixhub.com/model/zoo\">MaixHub 查看</a>或<a href=\"https://maixhub.com/model/zoo/share\">上传分享</a>能直接部署到边缘设备的模型\n</div>\n\n<div id=\"deploy_items\">\n    <a href=\"./k210.html\">\n        <div class=\"card\">\n            <img src=\"/hardware/zh/maix/assets/dk_board/maix_duino/maixduino_0.png\" alt=\"K210 模型转换和部署\">\n            <div class=\"card_info card_red\">\n                <h2>Maix-I 系列 K210</h2>\n                <div class=\"brief\">\n                    <div>高性价比带硬件 AI 加速的单片机</div>\n                    <div>1Tops@INT8，有限算子加速</div>\n                </div>\n            </div>\n        </div>\n    </a>\n    <a href=\"./v831.html\">\n        <div class=\"card\">\n            <img src=\"/hardware/assets/maixII/m2dock.jpg\" alt=\"V831 模型转换和部署\">\n            <div class=\"card_info card_blue\">\n                <h2>Maix-II 系列 v831</h2>\n                <div class=\"brief\">\n                    <div>高性价比带硬件 AI 加速，支持 Linux</div>\n                    <div>0.2Tops@INT8，有限算子加速</div>\n                </div>\n            </div>\n        </div>\n    </a>\n    <a href=\"./tinymaix.html\">\n        <div class=\"card\" style=\"background-color: #fafbfe\">\n            <img src=\"../../assets/m0_small.png\" alt=\"TinyMaix 模型转换和部署\">\n            <div class=\"card_info card_green\">\n                <h2>TinyMaix 平台</h2>\n                <div class=\"brief\">\n                    <div>单片机通用，为各种指令集优化</div>\n                    <div>算力具体看硬件 CPU，有限算子加速</div>\n                </div>\n            </div>\n        </div>\n    </a>\n    <a href=\"./ax-pi.html\">\n        <div class=\"card\" style=\"background-color: #fafbfe\">\n            <img src=\"../../assets/maix-iii-small.png\" alt=\"AXera-Pi 模型转换和部署\">\n            <div class=\"card_info card_purple\">\n                <h2>Maix-III 系列 AXera-Pi</h2>\n                <div class=\"brief\">\n                    <div>高算力、独特 AI-ISP 影像系统</div>\n                    <div>最高 3.6Tops@INT8，丰富算子支持</div>\n                </div>\n            </div>\n        </div>\n    </a>\n</div>\n\n<style>\n#deploy_items {\n    display: flex;\n    justify-content: space-evenly;\n    flex-wrap: wrap;\n    margin: 0 -10px;\n}\n#deploy_items a:hover {\n    background-color: transparent;\n}\n#deploy_items > a {\n    margin: 1em;\n}\n.card {\n    display: flex;\n    flex-direction: column;\n    justify-content: space-between;\n    align-items: center;\n    box-shadow: 5px 6px 20px 4px  rgba(0, 0, 0, 0.1);\n    border-radius: 0.6rem;\n    transition: 0.4s;\n    background: white;\n}\n.card:hover {\n    box-shadow: 5px 6px 40px 4px  rgba(0, 0, 0, 0.1);\n    scale: 1.05;\n}\n.card_info {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    border-radius: 0 0 0.6rem 0.6rem;\n}\n.card img {\n    height: 10em;\n    width: 14em;\n    object-fit: cover;\n}\n.card_info > h2 {\n    font-size: 1.2em;\n    margin: 0.2em;\n    padding: 0.2em 1em;\n}\n.card_info > .brief {\n    margin: 0.2em;\n    padding: 0.2em 1em;\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n}\n.card_red {\n    background-color: #ffcdd2;\n    color: #cf4f5a;\n}\n.card_blue {\n    background-color: #90caf9;\n    color: #105aa9;\n}\n.card_green {\n    background-color: #b2dfdb;\n    color: #009688;\n}\n.card_purple {\n    background-color: #d1c4e9;\n    color: #673ab7;\n}\n#maixhub {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    margin: 1em 0;\n    width: 100%;\n    background-color: #f5f5f5;\n    color: #727272;\n    border-radius: 0.6rem;\n    padding: 1em;\n}\n.dark #maixhub {\n    background-color: #2d2d2d;\n    color: #bfbfbf;\n}\n.dark .card_blue {\n    background-color: #003c6c;\n    color: #ffffffba;\n}\n.dark .card_red {\n    background-color: #5a0000;\n    color: #ffffffba;\n}\n.dark .card_green {\n    background-color: #004e03;\n    color: #ffffffba;\n}\n.dark .card_purple {\n    background-color: #370040;\n    color: #ffffffba;\n}\n</style>"}, "/ai/zh/deploy/ax-pi.html": {"title": "部署模型到 Maix-III(M3) 系列 AXera-Pi 开发板", "content": "---\ntitle: 部署模型到 Maix-III(M3) 系列 AXera-Pi 开发板\ndate: 2022-09-21\n#update:\n#  - date: 2022-09-30\n#    author: neucrack\n#    content: 初版文档\n---\n\n<div id=\"title_card\">\n    <div class=\"card\" style=\"background-color: #fafbfe\">\n        <img src=\"../../assets/maix-iii-small.png\" alt=\"AXera-Pi 模型转换和部署\">\n        <div class=\"card_info card_purple\">\n            <div class=\"title\">Maix-III 系列之 AXera-Pi（爱芯派）</div>\n            <div class=\"brief\">\n                <div>高算力、独特 AI-ISP 影像系统</div>\n                <div>最高 3.6Tops@INT8，丰富算子支持</div>\n            </div>\n        </div>\n    </div>\n</div>\n<style>\n#title_card {\n    width:100%;\n    text-align:center;\n    background-color: #fafbfe;\n    margin-bottom: 1em;\n}\n#title_card img {\n  max-height: 20em;\n}\n.card_purple {\n    background-color: #d1c4e9;\n    color: #673ab7;\n}\n.dark .card_purple {\n    background-color: #370040;\n    color: #ffffffba;\n}\n.title {\n    font-size: 1.5em;\n    font-weight: 800;\n    padding: 0.8em;\n}\n</style>\n\n> 有任何想法或者修改建议，欢迎留言或者直接点击右上角`编辑本页`进行修改后提交 PR\n\n> [MaixHub](https://maixhub.com/model/zoo) 模型库有 AXera-Pi 能直接运行的模型，可以直接下载使用，也欢迎上传分享你的模型~\n\n[点击可以查看 Maix-III(M3) 系列 AXera-Pi 开发板详情和基础使用文档](/hardware/zh/maixIII/index.html)\n\n要部署模型到`AXera-Pi`，需要将模型量化到 INT8，减小模型大小的同时提高运行速度，一般采用 `PTQ`(训练后量化)的方式量化模型，步骤：\n**1.** 准备好浮点模型。\n**2.** 用模型量化和格式转换工具转换成 AXera-Pi 支持的格式，这里工具使用爱芯官方提供的 [pulsar](https://pulsar-docs.readthedocs.io) 。\n> 本文提供了快速入门和流程向导，强烈建议先通看一遍，再查看 pulsar 文档获得更多具体内容。\n**3.** 在 AXera-Pi 上运行模型。\n\n## 准备浮点模型\n\n使用 `Pytorch` 或者 `TensorFlow` 训练好模型， 将模型保存为 `onnx` 格式备用。\n\n需要注意只能使用 `AXera-Pi` 所支持的算子，见[算子支持列表](https://pulsar-docs.readthedocs.io/zh_CN/latest/appendix/op_support_list.html)。\n\n对于某些网络，可能需要将后处理剥离出来，使用`CPU`处理。\n\n### 举例\n\n比如我们用一个 PyTorch Hub 上面的 [mobilenetv2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/):\n```python\nimport torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\nmodel.eval()\n```\n\n导出为 `onnx` 格式:\n```python\nx = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(model, x, \"mobilenetv2.onnx\", export_params = True, verbose=True, opset_version=11)\n```\n有些模型可以使用 `onnxsim` 精简下网络结构（这个模型实际上不需要）：\n```\npip install onnx-simplifier\npython -m onnxsim mobilenetv2.onnx mobilenetv2-sim.onnx\n```\n\n\n## 模型量化和格式转换\n\n### 安装`docker`\n\n[安装教程](https://docs.docker.com/engine/install/)\n\n安装好后查看\n```\ndocker --version\n```\n\n`Linux`下添加当前用户到 `docker`组，这样就不需要使用 `sudo` 运行`docker`命令了。可以用以下命令来添加：\n```shell\nsudo gpasswd -a $USER docker\nnewgrp docker\n```\n\n### 下载转换工具\n\n转换工具是以 docker 镜像的方式提供的，下载后用 docker 加载镜像即可。\n\n| 下载站点 | 简介 | 使用方法 |\n| :--- | :--- | :--- |\n| [dockerhub](https://hub.docker.com/r/sipeed/pulsar/tags) | 执行命令即可在线下载 | `docker pull sipeed/pulsar` |\n| dockerhub 国内镜像 | 中国国内下载加速 | 1. 编辑`/etc/docker/daemon.json`添加`\"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\"],`(也可以用其它镜像比如[阿里云](https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors))<br>2. 然后 `docker pull sipeed/pulsar` 拉取镜像 |\n\n拉取完成后使用`docker images`命令可以看到`sipeed/pulsar:latest`镜像\n> 注意这里镜像名叫 `sipeed/pulsar`， 在文档里面有些地方可能是 `axera/neuwizard`，是等效的，只是名字不同\n\n然后创建容器：\n```shell\ndocker run -it --net host --rm --shm-size 32g -v $PWD:/data sipeed/pulsar\n```\n> * 这里`--shm-size`共享内run大小根据你的电脑内存大小设置。\n> * 不用`--rm`会保留容器，建议加个`-name xxx`来命名容器，下次通过`docker start xxx && docker attach xxx`进入容器\n> * `-v 宿主机路径:/data`是把宿主机的目录挂载到容器的`/data`目录，这样就可以在容器里面直接操作宿主机的文件了。\n\n创建容器后会自动进入容器，使用 `pulsar -h` 命令可以看到相关命令。\n\n### 进行模型量化和转换\n\n然后看 [pulsar](https://pulsar-docs.readthedocs.io) 文档中的转换命令以及配置文件方法进行模型量化和格式转换。\n> 注意 `AXera-Pi` 使用了虚拟 NPU 的概念来划分算力，以将实现全部算力给 NPU 或者 NPU 和 AI-ISP 各分一半。\n\n#### 举例\n\n仍然以 `mobilenetv2` 为例：\n* 根据`pulsar`文档，准备好配置文件`config_mobilenetv2.prototxt`, (具体格式说明见[配置文件详细说明](https://pulsar-docs.readthedocs.io/zh_CN/latest/test_configs/config.html)),内容如下：\n.. details:: config_mobilenetv2.prototxt\n    ```protobuf\n    # 基本配置参数：输入输出\n    input_type: INPUT_TYPE_ONNX\n    output_type: OUTPUT_TYPE_JOINT\n\n    # 硬件平台选择\n    target_hardware: TARGET_HARDWARE_AX620\n\n    # CPU 后端选择，默认采用 AXE\n    cpu_backend_settings {\n        onnx_setting {\n            mode: DISABLED\n        }\n        axe_setting {\n            mode: ENABLED\n            axe_param {\n                optimize_slim_model: true\n            }\n        }\n    }\n\n    # onnx 模型输入数据类型描述\n    src_input_tensors {\n        color_space: TENSOR_COLOR_SPACE_RGB\n    }\n\n    # joint 模型输入数据类型设置\n    dst_input_tensors {\n        color_space: TENSOR_COLOR_SPACE_RGB\n    }\n\n    # neuwizard 工具的配置参数\n    neuwizard_conf {\n        operator_conf {\n            input_conf_items {\n                attributes {\n                    input_modifications {\n                        # y = x * (slope / slope_divisor) + (bias / bias_divisor)\n                        # 这里就是先把数据归一到[0, 1]\n                        affine_preprocess {\n                            slope: 1\n                            slope_divisor: 255\n                            bias: 0\n                        }\n                    }\n                    input_modifications {\n                        # y = (x - mean) / std\n                        # 按照训练的时候的参数标准化\n                        input_normalization {\n                            mean: [0.485,0.456,0.406]  ## 均值， 注意这里的顺序根据 src_input_tensors.color_space 而定， 比如这里是 [R G B]\n                            std: [0.229,0.224,0.255]   ## 方差\n                        }\n                    }\n                }\n            }\n        }\n        dataset_conf_calibration {\n            path: \"imagenet-1k-images-rgb.tar\" # 设置 PTQ 校准数据集路径\n            type: DATASET_TYPE_TAR         # 数据集类型：tar 包\n            size: 256                      # 量化校准过程中实际使用的图片张数\n            batch_size: 1\n    }\n    }\n\n    # pulsar compiler 的 batch size 配置参数\n    pulsar_conf {\n        ax620_virtual_npu: AX620_VIRTUAL_NPU_MODE_111 # 使用虚拟 NPU， NPU 和 AI-ISP 各分一半， 111 代表 NPU\n                        #  AX620_VIRTUAL_NPU_MODE_0   # 不使用虚拟 NPU， 全部算力给 NPU\n                        #  AX620_VIRTUAL_NPU_MODE_112 # 使用虚拟 NPU， NPU 和 AI-ISP 各分一半， 112 代表 AI-ISP 特用，千万别乱设置\n        batch_size: 1\n    }\n    ```\n    > 注意这里的预处理要和训练模型时相同，即 [mobilenetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) 的预处理说明，这里是先归一再减`mean`除以`std`。\n    > 以及`imagenet-1k-images-rgb.tar`是从训练集中抠出来的一部分图片，用于量化校准，在这里训练集是`imagenet`，测试用可以在[百度云](https://pan.baidu.com/s/1TiZSIm0fpqbLn-2qLBX58g?pwd=1rpb)或者[github](https://github.com/sipeed/sipeed_wiki/releases/download/v0.0.0/imagenet-1k-images-rgb.tar)下载，也可以你自己从[imagenet](https://www.image-net.org/)里面抠图出来。TODO: 需不需要手动先缩放到输入尺寸？\n    > 这里 `ax620_virtual_npu` 设置为了 `AX620_VIRTUAL_NPU_MODE_111`，这很重要，如果要用摄像头的话，必须要设置为这个， 否则可能会初始化失败，因为默认使用摄像头开启了 `AI-ISP` 会启用虚拟 NPU 并使用另一半算力，为保证我们第一次能使用，先这样设置，后面会详细介绍。\n\n然后在 `docker`容器里面执行（注意文件通过前面`docker run`的`-v`参数指定了挂载宿主机的目录到`docker`容器里面，直接拷贝到宿主机的目录就好了）：\n```\npulsar build --input mobilenetv2.onnx --output mobilenetv2.joint --config config_mobilenetv2.prototxt --output_config out_config_mobilenet_v2.prototxt\n```\n耐心等待，可能需要一小会儿时间，就会得到转换的模型结果`mobilenetv2.joint`了\n\n### 在 docker 中使用 GPU 进行模型量化和格式转换\n\n默认 docker 不能使用显卡驱动，但是有要用的话方法也不难：\n* 宿主机正常安装显卡驱动，比如 `ubuntu` 直接可以在包管理器中安装\n* 按照 [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) 的说明安装， 然后测试是否可用：\n```\ndocker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi\n```\n这会执行 `nvidia-smi` 命令，就可以看到映射到 docker 里面的显卡信息了。\n* 在需要使用显卡的容器创建时加上 `--gpus all` 参数来将所有显卡驱动映射到容器内，也可以指定特定显卡编号 `--gpus '\"device=2,3\"'`, 比如：\n```\ndocker run -it --net host --rm --gpus all --shm-size 32g -v $PWD:/data sipeed/pulsar\n```\n\n\n\n## 在 AXera-Pi 上测试运行模型\n\n按照文档转换模型后，将模型通过 `scp` 或者 `adb` 传到 `AXera-Pi` 上，使用文档中的模型测试运行命令运行模型即可。\n\n### 举例\n\n仍然以`mobilenetv2`为例：\n测试图片保存到`cat.jpg`：\n<img src=\"../../assets/cat.jpg\" style=\"max-height: 20em;\">\n\n* 先在电脑上跑一下和`onnx`的结果对比:\n```\npulsar run mobilenetv2.onnx mobilenetv2.joint --input cat.jpg --config out_config_mobilenet_v2.prototxt --output_gt gt\n```\n得到余弦距离，这里为 `0.9862` ，说明 `joint` 模型和 `onnx` 模型的输出结果相似度 `98.62%`，在可以接受的范围内，如果值太小，则表示量化过程中出现了误差，需要考虑是不是设置有误或者量化输入数据有误或者模型设计有问题。\n```log\nLayer: 536  2-norm RE: 17.03%  cosine-sim: 0.9862\n```\n\n* 拷贝模型到 `AXera-Pi` 上直接跑一下(通过 `scp` 命令拷贝 `joint` 格式模型文件到开发板）：\n在板子上运行模型：\n```\ntime run_joint mobilenetv2.joint --repeat 100 --warmup 10\n```\n可以看到模型运行时间 `2.1ms`，这里我们没有启用虚拟 NPU，如果启用了虚拟 NPU，则时间加倍为 `4ms`。以及 `overhead 250.42 ms` 即其它耗时（比如 模型加载 内存分配 等）。\n```\nRun task took 2143 us (99 rounds for average)\n        Run NEU took an average of 2108 us (overhead 9 us)\n\n```\n如果要测试输入，需要先将图片转换为二进制内容，`HWC + RGB` 排列，用 `--data` 指定二进制文件。\n\n.. details::转换为二进制文件脚本\n    ```python\n    from PIL import Image\n    import sys\n    out_path = sys.argv[2]\n    img = Image.open(sys.argv[1])\n    img = img.convert('RGB')\n    img = img.resize((224, 224))\n    rgb888 = img.tobytes()\n    with open(out_path, \"wb\") as f:\n        f.write(rgb888)\n    ```\n    执行`python convert.py cat.jpg cat.bin`，得到`cat.bin`文件。\n```\nrun_joint mobilenetv2.joint --data cat.bin --bin-out-dir ./\n```\n然后目录下会生成一个`bin`文件，大小是`4000`个字节，即`1000`个`float32`，可以用`python`加载找出最大值\n```python\nout = np.fromfile(\"536.bin\", dtype=np.float32)\nprint(out.argmax(), out.max())\n```\n得到结果`282 8.638927`，在[labels](https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt)中找到下标第`282`即`283`行，是`tiger cat`，结果和直接电脑运行浮点模型结果`282 9.110947`也一致，虽然值有略微差异，但是在可以接受的范围内。\n> 注意这里没有进行`softmax`计算，`out.max()`值不是概率值。\n\n## 编写代码运行模型\n\n要正式地将模型跑起来，你可能需要需要修改代码，更改输入预处理或者增加后处理，目前提供 `C/C++` SDK，代码参考 [ax-samples](https://github.com/AXERA-TECH/ax-samples)，可以交叉编译，也可以直接在 `AXera-Pi` 上编译。\n\n运行分类模型的代码在[ax_classification_steps.cc](https://github.com/AXERA-TECH/ax-samples/blob/main/examples/ax_classification_steps.cc)，按照仓库的编译说明编译后得到`build/bin/install/ax_classification`可执行文件，拷贝到开发板执行\n```\n./ax_classification -m mobilenetv2.joint -i cat.jpg\n```\n> 在代码中使用了 `opencv` 读取了图片，格式为 `BGR`，在运行模型时会根据转换模型时的配置自动判断是否转成 `RGB`，所以直接用了 `mw::prepare_io` 拷贝 `BGR` 的图到输入缓冲区了，后面就交给底层库处理了。\n\n如果你的模型不是简单的分类模型，那你可能需要在模型推理结束后添加后处理的代码以达到解析结果的目的。\n\n## 使用摄像头和屏幕\n\n到此，模型单独运行已经走通了，如果要基于此做应用，因为是 `Linux`，有很多通用的开发方法和库，如果你希望将摄像头和屏幕联合起来使用，可以使用[libmaix](https://github.com/sipeed/libmaix)，或者你也可以直接使用[axpi_bsp_sdk](https://github.com/sipeed/axpi_bsp_sdk)进行开发（难度会比较大一点）。\n* 看[SDK 开发说明](/hardware/zh/maixIII/ax-pi/sdk.html) 编译并执行[摄像头屏幕显示例程](https://github.com/sipeed/libmaix/tree/release/examples/axpi)，因为仓库都在`github`，所以最好是有个好的代理服务器。\n* 将模型运行代码合并到例程中，有个问题要十分注意，如果要使用摄像头，默认`AI-ISP`会打开（暂未提供关闭的方式，后面更新TODO:），**所以模型转换的时候要指定模型为虚拟NPU运行，即配置文件中设置`ax620_virtual_npu: AX620_VIRTUAL_NPU_MODE_111`，否则初始化会失败**。\n> 可以直接使用[1000分类例程](https://github.com/sipeed/libmaix/tree/release/examples/axpi_classification_cam).\n> 在板子编译通过后执行`./dist/axpi_classification_cam -m mobilenetv2.joint` 即可开始识别。模型也可以到[MaixHub 模型库下载](https://maixhub.com/model/zoo/89)\n\n\n## QAT 量化 和其它优化方法\n\n`QAT`(Quantization aware training)即量化感知训练，和 `PTQ` 在对训练好的模型进行浏览量化的做法不同，`QAT` 是在训练时就模拟量化推理，以减少量化误差， 和训练后量化 `PTQ` 相比，有更高的精度，但是过程会更复杂，不建议一开始就使用。\n\n更多详情看[superpulsar](https://pulsar-docs.readthedocs.io)， 文档会持续更新，如果你擅长这方面，也欢迎点击右上角`编辑本页`在这里添加说明。\n\n\n## 其它参考\n\n> 欢迎贴上你的分享，点击右上角`编辑本页`添加。\n\n* [爱芯元智AX620A部署yolov5 6.0模型实录](https://zhuanlan.zhihu.com/p/569083585)\n* [AX620A运行yolov5s自训练模型全过程记录（windows）](http://t.csdn.cn/oNeYG)"}, "/ai/zh/deploy/v831.html": {"title": "将模型部署到 V831", "content": "---\ntitle: 将模型部署到 V831\ndate: 2022-09-15\n---\n\n<div id=\"title_card\">\n    <div class=\"card\">\n        <img src=\"/hardware/assets/maixII/m2dock.jpg\" alt=\"V831 模型转换和部署\">\n        <div class=\"card_info card_blue\">\n            <div class=\"title\">Maix-II 系列 v831</div>\n            <div class=\"brief\">\n                <div>高性价比带硬件 AI 加速，支持 Linux</div>\n                <div>0.2Tops@INT8，有限算子加速</div>\n            </div>\n        </div>\n    </div>\n</div>\n<style>\n#title_card {\n    width:100%;\n    text-align:center;\n    background-color: white;\n    margin-bottom: 1em;\n}\n#title_card img {\n  max-height: 20em;\n}\n.card_blue {\n    background-color: #90caf9;\n    color: #105aa9;\n}\n.dark .card_blue {\n    background-color: #003c6c;\n    color: #ffffffba;\n}\n.title {\n    font-size: 1.5em;\n    font-weight: 800;\n    padding: 0.8em;\n}\n</style>\n\n\n## 制作浮点模型\n\n对于 V831， 强烈推荐使用`Pytorch`训练模型，因为模型转换工具对其支持较好。\n\n这里直接使用 pytorch hub 的预训练模型为例。\n\n这里省略了模型定义和训练过程， 直接使用 pytorch hub 的 resnet18 预训练模型进行简单介绍：\nhttps://pytorch.org/hub/pytorch_vision_resnet/\n\n\n注意 V831 支持的算子有限，具体请在 [MaixHub](https://maixhub.com/) 点击`工具箱->模型转换->v831`中的文档查看。\n\n\n## 在 PC 端测试模型推理\n\n\n根据上面链接的使用说明， 使用如下代码可以运行模型\n\n其中， label 下载： https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n```python\nimport os\nimport torch\nfrom torchsummary import summary\n\n\n## model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n\nmodel.eval()\n\ninput_shape = (3, 224, 224)\nsummary(model, input_shape, device=\"cpu\")\n\n## test image\nfilename = \"out/dog.jpg\"\nif not os.path.exists(filename):\n    if not os.path.exists(\"out\"):\n        os.makedirs(\"out\")\n    import urllib\n    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n    try: urllib.URLopener().retrieve(url, filename)\n    except: urllib.request.urlretrieve(url, filename)\n\nprint(\"test image:\", filename)\n\n## preparing input data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\ninput_image = Image.open(filename)\n# input_image.show()\npreprocess = transforms.Compose([\n    transforms.Resize(max(input_shape[1:3])),\n    transforms.CenterCrop(input_shape[1:3]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\n\nprint(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor)))\n\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n## forward model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\n\nwith torch.no_grad():\n    output = model(input_batch)\n\n## result    \n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n# print(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nmax_1000 = torch.nn.functional.softmax(output[0], dim=0)\nmax_idx = int(torch.argmax(max_1000))\nwith open(\"imagenet_classes.txt\") as f:\n    labels = f.read().split(\"\\n\")\nprint(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx]))\n```\n\n运行后 结果：\n```\nUsing cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 44.59\nEstimated Total Size (MB): 107.96\n----------------------------------------------------------------\nout/dog.jpg\ntensor(2.6400) tensor(-2.1008)\nidx:258, name:Samoyed, Samoyede\n```\n\n可以看到模型有 `11,689,512`的参数， 即 `11MiB`左右， 这个大小也就几乎是实际在 831 上运行的模型的大小了\n\n## 将模型转换为 V831 能使用的模型文件\n\n转换过程如下：\n\n### 使用 Pytorch 将模型导出为 `onnx`模型， 得到`onnx`文件\n\n```python\ndef torch_to_onnx(net, input_shape, out_name=\"out/model.onnx\", input_names=[\"input0\"], output_names=[\"output0\"], device=\"cpu\"):\n    batch_size = 1\n    if len(input_shape) == 3:\n        x = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype=torch.float32, requires_grad=True).to(device)\n    elif len(input_shape) == 1:\n        x = torch.randn(batch_size, input_shape[0], dtype=torch.float32, requires_grad=False).to(device)\n    else:\n        raise Exception(\"not support input shape\")\n    print(\"input shape:\", x.shape)\n    # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params=True)\n    torch.onnx.export(net, x, out_name, export_params=True, input_names = input_names, output_names=output_names)\n\n\nonnx_out=\"out/resnet_1000.onnx\"\nncnn_out_param = \"out/resnet_1000.param\"\nncnn_out_bin = \"out/resnet_1000.bin\"\ninput_img = filename\n\ntorch_to_onnx(model, input_shape, onnx_out, device=\"cuda:0\")\n\n```\n\n如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字\n\n## 使用 `onnx2ncnn` 工具将`onnx`转成`ncnn`模型，得到一个`.param`文件和一个`.bin`文件\n\n>! 这一步可以跳过。\n\n> 按照[ncnn项目](https://github.com/Tencent/ncnn)的编译说明编译，在`build/tools/onnx`目录下得到`onnx2ncnn`可执行文件\n\n```python\ndef onnx_to_ncnn(input_shape, onnx=\"out/model.onnx\", ncnn_param=\"out/conv0.param\", ncnn_bin = \"out/conv0.bin\"):\n    import os\n    # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir\n    cmd = f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\"\n    os.system(cmd)\n    with open(ncnn_param) as f:\n        content = f.read().split(\"\\n\")\n        if len(input_shape) == 1:\n            content[2] += \" 0={}\".format(input_shape[0])\n        else:\n            content[2] += \" 0={} 1={} 2={}\".format(input_shape[2], input_shape[1], input_shape[0])\n        content = \"\\n\".join(content)\n    with open(ncnn_param, \"w\") as f:\n        f.write(content)\n\nonnx_to_ncnn(input_shape, onnx=onnx_out, ncnn_param=ncnn_out_param, ncnn_bin=ncnn_out_bin)\n```\n\n## 使用全志提供的`awnn`工具将`ncnn`模型进行量化到`int8`模型\n\n在 [MaixHub](https://maixhub.com/) 点击`工具箱->模型转换->v831`进入模型转换页面， 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做）\n\n在转换页面有更多的转换说明，可以获得更多详细的转换说明\n\n这里有几组参数：\n* 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值  - mean ) / std`, `awnn`对输入的处理是 `(输入值 - mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给`awnn`量化工具经过` (输入值 - mean ) * norm` 计算后的值范围一致既可。 比如 这里打印了实际数据的输入范围是`[-2.1008, 2.6400]`， 是代码中`preprocess` 对象处理后得到的，即`x = (x - mean) / std` ==> `(0-0.485)/0.229 = -2.1179`, 到`awnn`就是`x = (x - mean_2*255) * (1 / std * 255)` 即 `mean2 = mean * 255`, `norm = 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result#pre-process)。\n所以我们这里可以设置 均值为 `0.485 * 255 = 123.675`， 设置 归一化因子为`1/ (0.229 * 255) = 0.017125`， 另外两个通道同理，但是目前 awnn 只能支持三个通道值一样。。。所以填`123.675, 123.675, 123.675`，`0.017125, 0.017125, 0.017125` 即可，因为这里用了`pytorch hub`的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下\n\n* 图片输入层尺寸（问不是图片怎么办？貌似 awnn 暂时只考虑到了图片。。）\n* RGB 格式： 如果训练输入的图片是 RGB 就选 RGB\n* 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手\n\n自己写的其它模型转换如果失败，多半是啥算子不支持，需要在 使用说明里面看支持的 算子，比如之前的版本view、 flatten、reshape  都不支持所以写模型要相当小心， 现在的版本会支持 flatten reshape 等 CPU 算子\n\n如果不出意外， 终于得到了量化好的 awnn 能使用的模型， `*.param` 和 `*.bin`\n\n\n\n## 使用模型，在v831上推理\n\n可以使用 python 或者 C 写代码，以下两种方式\n\n### MaixPy3\n\npython 请看[MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/)\n\n不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了：\n```\npip install --upgrade maixpy3\n```\n\n然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）：\n\nhttps://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py\n\nlabel 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py\n\n```python\nfrom maix import nn\nfrom PIL import Image, ImageDraw\nfrom maix import camera, display\n\ntest_jpg = \"/root/test_input/input.jpg\"\nmodel = {\n    \"param\": \"/root/models/resnet_awnn.param\",\n    \"bin\": \"/root/models/resnet_awnn.bin\"\n}\n\ncamera.config(size=(224, 224))\n\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (1, 1, 1000)\n    },\n    \"first_layer_conv_no_pad\": False,\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n}\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nprint(\"-- read image\")\nimg = Image.open(test_jpg)\nprint(\"-- read image ok\")\nprint(\"-- forward model with image as input\")\nout = m.forward(img, quantize=True)\nprint(\"-- read image ok\")\nprint(\"-- out:\", out.shape)\nout = nn.F.softmax(out)\nprint(out.max(), out.argmax())\n\nfrom classes_label import labels\nwhile 1:\n    img = camera.capture()\n    if not img:\n        time.sleep(0.02)\n        continue\n    out = m.forward(img, quantize=True)\n    out = nn.F.softmax(out)\n    msg = \"{:.2f}: {}\".format(out.max(), labels[out.argmax()])\n    print(msg)\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 0), msg, fill=(255, 0, 0))\n    display.show(img)\n```\n\n\n### C语言 SDK， libmaix\n\n访问这里，按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet\n\n上传编译成功后`dist`目录下的所有内容到 `v831`, 然后执行`./start_app.sh`即可\n\n\n## 参考\n\n* [在V831上（awnn）跑 pytorch resnet18 模型](https://neucrack.com/p/358)"}, "/ai/zh/deploy/tinymaix.html": {"title": "使用 TinyMaix 将模型部署到单片机", "content": "---\ntitle: 使用 TinyMaix 将模型部署到单片机\ndate: 2022-09-15\n---\n\n<div id=\"title_card\">\n    <div class=\"card\" style=\"background-color: #fafbfe\">\n        <img src=\"../../assets/m0_small.png\" alt=\"TinyMaix 模型转换和部署\">\n        <div class=\"card_info card_green\">\n            <div class=\"title\">TinyMaix 平台</div>\n            <div class=\"brief\">\n                <div>单片机通用，为各种指令集优化</div>\n                <div>算力具体看硬件 CPU，有限算子加速</div>\n            </div>\n        </div>\n    </div>\n</div>\n<style>\n#title_card {\n    width:100%;\n    text-align:center;\n    background-color: #fafbfe;\n    margin-bottom: 1em;\n}\n#title_card img {\n  max-height: 20em;\n}\n.card_green {\n    background-color: #b2dfdb;\n    color: #009688;\n}\n.dark .card_green {\n    background-color: #004e03;\n    color: #ffffffba;\n}\n.title {\n    font-size: 1.5em;\n    font-weight: 800;\n    padding: 0.8em;\n}\n</style>\n\n[TinyMaix](https://github.com/sipeed/TinyMaix) 是针对小算力小内存的芯片设计的轻量级推理框架，甚至能在`2KB`内存的`Arduino ATmega328`单片机上运行`MNIST`，对各种架构的单片机都提供了支持和优化，包括 RISC-V、ARM Cortex-M 等。\n\n\n详细使用方法请看[TinyMaix 官方仓库](https://github.com/sipeed/TinyMaix)"}}